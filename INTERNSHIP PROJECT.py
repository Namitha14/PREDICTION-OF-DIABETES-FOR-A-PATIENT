# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SZk6hOS172mjc-qbUNGs7HD6j2yEZksA
"""

import pandas as pd
df=pd.read_csv("diabetes.csv",header=None)
h=['Pregnancies','Glucose','BP','SkinThickness','Insulin','BMI','Diabetesfunc','Age','Outcome']
df.columns=h
df

#description of data
df.describe()

df.info()

import matplotlib.pyplot as plt
plt.figure(figsize=(15,75))
for i in range(len(h)):
  plt.subplot(19,3,i+1)
  plt.hist(df[h[i]])
  plt.title(h[i])

df['Outcome'].value_counts()

import seaborn as sns
sns.countplot(df['Outcome'])

X=df.drop(['Outcome'],axis=1)
Y=df['Outcome']
#KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
ts=[]
tr=[]
for i in range(1,20):
  kmodel=KNeighborsClassifier(n_neighbors=i)
  xtrain,xtest,ytrain,ytest=train_test_split(X,Y,random_state=66)
  kmodel.fit(xtrain,ytrain)
  Yp_train=kmodel.predict(xtrain)
  Yp_test=kmodel.predict(xtest)
  tr.append(((ytrain==Yp_train).sum())/len(xtrain))
  ts.append(((ytest==Yp_test).sum())/len(xtest))
plt.plot(range(1,20),tr)
plt.plot(range(1,20),ts,color='red')

X=df.drop(['Outcome'],axis=1)
Y=df['Outcome']
#KNN
from sklearn.neighbors import KNeighborsClassifier
kmodel=KNeighborsClassifier(n_neighbors=11)
kmodel.fit(X,Y)
Yp=kmodel.predict(X)
knn=((Y==Yp).sum()/len(df))*100
((Y==Yp).sum()/len(df))*100



#linearregression
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
lmodel=LinearRegression()
lmodel.fit(X,Y)
Yp2=lmodel.predict(X)
from sklearn.metrics import mean_absolute_error
#print(mean_absolute_error(Y,Yp2))
linearreg0=(lmodel.score(xtrain,ytrain))*100
linearreg1=(lmodel.score(xtest,ytest))*100
print((lmodel.score(xtrain,ytrain))*100)
print((lmodel.score(xtest,ytest))*100)
lmodel.predict([[6,148,72,35,0,33.6,0.627,50]])
#plt.scatter(xtrain,ytrain,color="red")

#LogisticRegression
from sklearn.linear_model import LogisticRegression
lgr=LogisticRegression()
lgr.fit(xtrain,ytrain)
ypr=lgr.predict(xtrain)
from sklearn import metrics
cnf=metrics.confusion_matrix(ytrain,ypr)
print(cnf)
logisreg0=(lgr.score(xtrain,ytrain))*100
logisreg1=(lgr.score(xtest,ytest))*100
print(lgr.score(xtrain,ytrain))
#print(lgr.score(ypr,ytest))
print(lgr.score(xtest,ytest))
print("accuracy",metrics.accuracy_score(ytrain,ypr))
print("precision",metrics.precision_score(ytrain,ypr))
lgr.predict([[6,148,72,35,0,33.6,0.627,50]])

f,a=plt.subplots(figsize=(8,8))
a.imshow(cnf)
a.grid(False)
a.xaxis.set(ticks=(0,1),ticklabels=('predicted ','predicted '))
a.yaxis.set(ticks=(0,1),ticklabels=('actual ','actual '))
a.set_ylim(1.5,-0.5)
for i in range(2):
  for j in range(2):
    a.text(j,i,cnf[i,j],ha='center',va='center',color='red')
plt.show()

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),annot= True,fmt = '.0%')

from sklearn.metrics import classification_report
print(classification_report(ytrain,ypr))

#PolynomialRegression
from sklearn.preprocessing import PolynomialFeatures
pol=PolynomialFeatures(degree=2)
x=pol.fit_transform(X)
xtrain0,xtest0,ytrain0,ytest0=train_test_split(x,Y,random_state=6)
lmodel_p=LinearRegression()
lmodel_p.fit(xtrain0,ytrain0)
ypredict=lmodel_p.predict(xtest0)
polreg0=(lmodel_p.score(xtest0,ytest0))*100
print((lmodel_p.score(xtest0,ytest0))*100)
#print(lmodel_p.score(,ytest0))

#RandomForest
from sklearn.ensemble import RandomForestClassifier
nmodel=RandomForestClassifier(n_estimators=8,max_features='auto',min_samples_split=4,min_samples_leaf=5)
nmodel.fit(xtrain,ytrain)
yp=nmodel.predict(xtrain)
print(mean_absolute_error(yp,ytrain))
random0=(nmodel.score(xtrain,ytrain))*100
random1=(nmodel.score(xtest,ytest))*100
print((nmodel.score(xtrain,ytrain))*100)
print((nmodel.score(xtest,ytest))*100)
nmodel.predict([[10,101,76,48,180,32.9,0.171,63]])

#DecicisonTree
from sklearn.tree import DecisionTreeClassifier
dmodel= DecisionTreeClassifier(max_depth=2)
dmodel.fit(xtrain,ytrain)
yp1=dmodel.predict(xtrain)
print(mean_absolute_error(yp1,ytrain))
decision0=(dmodel.score(xtrain,ytrain))*100
decision1=(dmodel.score(xtest,ytest))*100
print((dmodel.score(xtrain,ytrain))*100)
print((dmodel.score(xtest,ytest))*100)
dmodel.predict([[10,101,76,48,180,32.9,0.171,63]])

from sklearn.tree import export_graphviz
dot_data=export_graphviz(dmodel,out_file=None,feature_names=xtrain.columns,filled=True)
from IPython.display import Image
import pydotplus
graph=pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())

tra=[]
tsa=[]
for i in range(1,11):
  dmodel1= DecisionTreeClassifier(max_depth=i)
  dmodel1.fit(xtrain,ytrain)
  yp2=dmodel.predict(xtrain)
  tra.append(dmodel1.score(xtrain,ytrain))
  tsa.append(dmodel1.score(xtest,ytest))
plt.plot(range(1,11),tra)
plt.plot(range(1,11),tsa,c="red")
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
#sc=StandardScaler()
#X_sc=sc.fit_transform(X)
trac=[]
tsac=[]
for i in range(1,8):
  pca=PCA(n_components=i)
  xpca=pca.fit_transform(X)
  xtrain1,xtest1,ytrain1,ytest1=train_test_split(xpca,Y,random_state=40)
  dmodel2= DecisionTreeClassifier(max_depth=2)
  dmodel2.fit(xtrain1,ytrain1)
  yp3=dmodel2.predict(xtrain1)
  trac.append(dmodel2.score(xtrain1,ytrain1))
  tsac.append(dmodel2.score(xtest1,ytest1))
plt.plot(range(1,8),trac)
plt.plot(range(1,8),tsac,c="red")
plt.show()

from sklearn.decomposition import PCA
etr=[]
ets=[]
for i in range(1,8):
  pca0=PCA(n_components=i)
  xpca0=pca0.fit_transform(X)
  x_train1,x_test1,y_train1,y_test1=train_test_split(xpca0,Y,random_state=6)
  knn= KNeighborsClassifier(n_neighbors=11)
  knn.fit(x_train1,y_train1)
  yk=knn.predict(x_train1)
  etr.append(knn.score(x_train1,y_train1))
  ets.append(knn.score(x_test1,y_test1))
plt.plot(range(1,8),etr)
plt.plot(range(1,8),ets,c="red")
plt.show()

pca1=PCA(n_components=3)
xpca1=pca1.fit_transform(X)
xtrain2,xtest2,ytrain2,ytest2=train_test_split(xpca1,Y,random_state=40)
dmodel3= DecisionTreeClassifier(max_depth=2)
dmodel3.fit(xtrain2,ytrain2)
yp4=dmodel3.predict(xtrain2)
pcat0=(dmodel3.score(xtrain2,ytrain2))*100
pcat1=(dmodel3.score(xtest2,ytest2))*100
print((dmodel3.score(xtrain2,ytrain2))*100)
print((dmodel3.score(xtest2,ytest2))*100)

#naive bayes
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
xtrain3,xtest3,ytrain3,ytest3=train_test_split(X,Y,test_size=0.45,random_state=6)
gnb.fit(xtrain3,ytrain3)
y_predict=gnb.predict(xtest3)
naive0=metrics.accuracy_score(ytest3,y_predict)*100
print(metrics.accuracy_score(ytest3,y_predict)*100)

#Standardscaler
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
xtrain4,xtest4,ytrain4,ytest4=train_test_split(X,Y,test_size=0.2,random_state=21)
sc=StandardScaler()
X_str=sc.fit_transform(xtrain4)
x_sts=sc.fit_transform(xtest4)
mlp=MLPClassifier(random_state=0,alpha=1,max_iter=100)
mlp.fit(X_str,ytrain4)
mlp.fit(x_sts,ytest4)
mlp0=(mlp.score(X_str,ytrain4))*100
mlp1=(mlp.score(x_sts,ytest4))*100
print((mlp.score(X_str,ytrain4))*100)
print((mlp.score(x_sts,ytest4))*100)

xlabel=['knn','linearregression','logisticregression','polynomialregression','decisiontree','RandomForest','pca','naivebayes','mlpclassifier']
ylabel0=[knn,linearreg0,logisreg0,polreg0,decision0,random0,pcat0,naive0,mlp0]
ylabel1=[knn,linearreg1,logisreg1,polreg0,decision1,random1,pcat1,naive0,mlp1]
#plt.figure(figsize=(12,10))
plt.title("Trained data accuracy")
plt.xticks(rotation=90)
plt.bar(xlabel,ylabel0,color="blue")
plt.show()

plt.title("Test data accuracy")
plt.xticks(rotation=90)
plt.bar(xlabel,ylabel1,color="blue")
plt.show()

